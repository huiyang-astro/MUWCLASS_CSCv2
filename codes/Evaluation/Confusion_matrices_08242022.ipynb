{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da3e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# import time\n",
    "import multiprocessing as mp\n",
    "import glob\n",
    "import sys  \n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from muwclass_library import loo_prepare, loo_train_and_classify, loo_save_res, plot_confusion_matrix,  confident_flag, confident_sigma\n",
    "from bokeh.io import show\n",
    "from bokeh.io import export_png\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ba8cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "accuracy for all classifications: 0.8860931655899354\n",
      "accuracy for confident classifications: 0.9701931150293871\n",
      "balanced accuracy for all classifications: 0.7022678504741028\n",
      "balanced accuracy for confident classifications: 0.7900553872136309\n"
     ]
    }
   ],
   "source": [
    "#dir_data = '/Volumes/PoplarII/Projects/2019/MUWCLASS_Project/ML/ML_pipelines_merge/Results/MUWCLASS_Classify_evaluation' # this is old results with wrong extinction correction (use band width rather than effetive wavelength)\n",
    "dir_data = '/media/orion51/PoplarII/Projects/2019/MUWCLASS_Project/ML/DATA/MUWCLASS_Classify_evaluation'\n",
    "class_labels = ['AGN','CV','HM-STAR','LM-STAR','HMXB','LMXB','NS','YSO']\n",
    "for test, title in zip(['final_08262022'], ['CM']):    \n",
    "    files = glob.glob(f'{dir_data}/LOO_{test}_results/*/classes.csv')\n",
    "    dfs = [pd.read_csv(f, usecols=range(len(class_labels))) for f in files]\n",
    "    print(len(dfs))\n",
    "    df_median   = pd.concat(dfs).groupby(level=0).median()\n",
    "    #df_1sig_up  = pd.concat(dfs).groupby(level=0).quantile(0.84135)\n",
    "    #df_1sig_low = pd.concat(dfs).groupby(level=0).quantile(0.15865)\n",
    "    df_2sig_up  = pd.concat(dfs).groupby(level=0).quantile(0.97725)\n",
    "    df_2sig_low = pd.concat(dfs).groupby(level=0).quantile(0.02275)\n",
    "    df_mean     = pd.concat(dfs).groupby(level=0).mean()\n",
    "    df_std      = pd.concat(dfs).groupby(level=0).std()\n",
    "\n",
    "\n",
    "    df_class = df_median.idxmax(axis=1)\n",
    "    df_prob = df_median.max(axis=1)\n",
    "    #df_prob_1sig_up  = pd.DataFrame(data=[df_1sig_up.values[i][np.argmax(np.array(df_median), axis=1)[i]]  for i in range(len(df_median))], columns=['Class_prob_1sig_upp'])\n",
    "    #df_prob_1sig_low = pd.DataFrame(data=[df_1sig_low.values[i][np.argmax(np.array(df_median), axis=1)[i]]  for i in range(len(df_median))], columns=['Class_prob_1sig_low'])\n",
    "    df_prob_2sig_up  = pd.DataFrame(data=[df_2sig_up.values[i][np.argmax(np.array(df_median), axis=1)[i]]  for i in range(len(df_median))], columns=['Class_prob_2sig_upp'])\n",
    "    df_prob_2sig_low = pd.DataFrame(data=[df_2sig_low.values[i][np.argmax(np.array(df_median), axis=1)[i]]  for i in range(len(df_median))], columns=['Class_prob_2sig_low'])\n",
    "    df_class_mean = df_mean.idxmax(axis=1)\n",
    "    df_prob_mean = df_mean.max(axis=1)\n",
    "    df_prob_e = pd.DataFrame(data=[df_std.values[i][np.argmax(np.array(df_mean), axis=1)[i]]  for i in range(len(df_std))], columns=['Class_prob_e'])\n",
    "\n",
    "    df_mean = df_mean.add_prefix('P_')\n",
    "    df_std  = df_std.add_prefix('e_P_')\n",
    "    df_median = df_median.add_prefix('P_median_')\n",
    "    df_2sig_up = df_2sig_up.add_prefix('P_2sig_upp_')\n",
    "    df_2sig_low = df_2sig_low.add_prefix('P_2sig_low_')\n",
    "\n",
    "    df = pd.concat([df_median, df_2sig_up, df_2sig_low, df_mean, df_std, df_class, df_prob, df_prob_2sig_up, df_prob_2sig_low, df_class_mean, df_prob_mean, df_prob_e, pd.read_csv(files[0]).name, pd.read_csv(files[0]).true_Class],axis=1).rename(columns={0:'Class_median',1:'Class_prob_median',2:'Class',3:'Class_prob'})\n",
    "    #print(df.columns)\n",
    "   \n",
    "    df_default = confident_flag(df, method = 'sigma-mean', class_cols=class_labels)\n",
    "    df_default = confident_sigma(df, class_cols=class_labels)\n",
    "    df_default.to_csv(f'{dir_data}/LOO_{test}_results/LOO_classes.csv',index=False)\n",
    "   \n",
    "\n",
    "    print('accuracy for all classifications:',accuracy_score(df_default.true_Class, df_default.Class))\n",
    "    print('accuracy for confident classifications:',accuracy_score(df_default.loc[df_default.conf_flag>0, 'true_Class'], df_default.loc[df_default.conf_flag>0, 'Class']))\n",
    "    print('balanced accuracy for all classifications:',balanced_accuracy_score(df_default.true_Class, df_default.Class))\n",
    "    print('balanced accuracy for confident classifications:',balanced_accuracy_score(df_default.loc[df_default.conf_flag>0, 'true_Class'], df_default.loc[df_default.conf_flag>0, 'Class']))\n",
    "   \n",
    "   \n",
    "   \n",
    "    CM = plot_confusion_matrix(df_default, title=f'recall {title}',classes=class_labels)\n",
    "    #show(CM)\n",
    "    conf_CM = plot_confusion_matrix(df_default[df_default.conf_flag>0], title=f'confident recall {title}',classes=class_labels,count_fraction=True,df_all = df_default)\n",
    "    CM_p = plot_confusion_matrix(df_default, title=f'precision {title}',classes=class_labels,cm_type='precision')\n",
    "    #show(CM)\n",
    "    conf_CM_p = plot_confusion_matrix(df_default[df_default.conf_flag>0], title=f'confident precision {title}',classes=class_labels,cm_type='precision',count_fraction=True,df_all = df_default)\n",
    "    export_png(CM, filename=f'./figs/{title}.png')\n",
    "    export_png(conf_CM, filename=f'./figs/{title}_conf.png')\n",
    "    export_png(CM_p, filename=f'./figs/{title}_precision.png')\n",
    "    export_png(conf_CM_p, filename=f'./figs/{title}_conf_precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "958b0475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for all classifications: 0.8860931655899354\n",
      "accuracy for confident classifications: 0.9701931150293871\n",
      "balanced accuracy for all classifications: 0.7022678504741028\n",
      "balanced accuracy for confident classifications: 0.7900553872136309\n",
      "f1_score for all classifications: 0.6811556981396667\n",
      "f1_score for confident classifications: 0.7949102762120428\n",
      "completeness: 0.8099285957157429\n",
      "balanced completeness: 0.5400939092091234\n"
     ]
    }
   ],
   "source": [
    "df_default = pd.read_csv(f'./LOO_classes.csv')\n",
    "   \n",
    "\n",
    "print('accuracy for all classifications:',accuracy_score(df_default.true_Class, df_default.Class))\n",
    "print('accuracy for confident classifications:',accuracy_score(df_default.loc[df_default.conf_flag>0, 'true_Class'], df_default.loc[df_default.conf_flag>0, 'Class']))\n",
    "print('balanced accuracy for all classifications:',balanced_accuracy_score(df_default.true_Class, df_default.Class))\n",
    "print('balanced accuracy for confident classifications:',balanced_accuracy_score(df_default.loc[df_default.conf_flag>0, 'true_Class'], df_default.loc[df_default.conf_flag>0, 'Class']))\n",
    "print('f1_score for all classifications:',f1_score(df_default.true_Class, df_default.Class, average='macro'))\n",
    "print('f1_score for confident classifications:',f1_score(df_default.loc[df_default.conf_flag>0, 'true_Class'], df_default.loc[df_default.conf_flag>0, 'Class'], average='macro'))\n",
    "\n",
    "\n",
    "print('completeness:',len(df_default.loc[df_default.conf_flag>0])/len(df_default))\n",
    "class_labels = ['AGN','CV','HM-STAR','HMXB','LM-STAR','LMXB','NS','YSO']\n",
    "class_completeness = [len(df_default.loc[(df_default.conf_flag>0) & (df_default.Class==c)]) /len(df_default.loc[(df_default.Class==c)]) for c in class_labels ]\n",
    "print('balanced completeness:',np.mean(class_completeness))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ac69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
